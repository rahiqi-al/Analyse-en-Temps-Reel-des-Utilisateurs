# ğŸ“Œ Plateforme de Streaming en Temps RÃ©el pour la Gestion et l'Analyse des Utilisateurs

Ce projet met en place un pipeline de donnÃ©es en temps rÃ©el pour ingÃ©rer, traiter et visualiser les donnÃ©es des utilisateurs. En utilisant l'API Random User, le systÃ¨me rÃ©cupÃ¨re des profils d'utilisateurs et les traite en quasi-temps rÃ©el grÃ¢ce Ã  Apache Kafka, PySpark Structured Streaming, MongoDB et Cassandra. Les rÃ©sultats finaux sont affichÃ©s sur un tableau de bord Streamlit pour le suivi et l'analyse.

## ğŸš€ FonctionnalitÃ©s

- **Streaming basÃ© sur Kafka** : Collecte et distribue efficacement les donnÃ©es utilisateur.
- **Traitement avec PySpark** : Transforme et agrÃ¨ge les donnÃ©es dynamiquement.
- **Stockage MongoDB & Cassandra** : Stockage flexible des documents et requÃªtes analytiques rapides.
- **Tableau de bord Streamlit** : Visualisation en direct des donnÃ©es brutes et agrÃ©gÃ©es.
- **RÃ©silience et scalabilitÃ©** : Gestion des Ã©checs d'ingestion avec des mÃ©canismes de reprise.

## âš™ï¸ Architecture du Projet

```
+-----------------------+
|   Random User API     |
|                       |
+----------+----------- +
           |
(1) Script d'Ingestion (Python) 
           v
+-----------------------+
|   Producteur Kafka    |
+----------+----------- +
           |
           v
+----------------------+
|  Cluster Kafka       |
| (Brokers, Zookeeper) |
+----------+-----------+
           |
           v
+-----------------------+
| PySpark Streaming     |
| (Traitement StructurÃ©)|
+----------+----------- +
   |        |        |
   |        |        | (4) Ã‰criture des agrÃ©gats
   |        |        | dans Cassandra
   |        |        |
   v        v        v
+--------+ +-------------+ +-----------------+
|MongoDB | |  Cassandra  | |   Monitoring    |
+--------+ +-------------+ +-----------------+
          |
          v
  (5) Tableau de bord Streamlit
```

## ğŸ“Š Flux de DonnÃ©es

1. **Ingestion des donnÃ©es** : Le script Python rÃ©cupÃ¨re les utilisateurs depuis l'API Random User.
2. **Kafka Producer** : Envoie les donnÃ©es utilisateur Ã  un topic Kafka.
3. **PySpark Streaming** : Lit les donnÃ©es de Kafka, les transforme et les stocke dans MongoDB & Cassandra.
4. **MongoDB** : Stocke les donnÃ©es brutes des utilisateurs.
5. **Cassandra** : Stocke les statistiques agrÃ©gÃ©es des utilisateurs.
6. **Tableau de bord Streamlit** : Affiche les mises Ã  jour en temps rÃ©el.

## ğŸ¯ Composants ClÃ©s

- **Producteur Kafka (Python & Requests)** : RÃ©cupÃ¨re les donnÃ©es utilisateur et les envoie Ã  Kafka.
- **Cluster Kafka** : Gestionnaire de messages pour le streaming en temps rÃ©el.
- **PySpark Streaming** : Traitement et enrichissement des donnÃ©es.
- **MongoDB** : Stockage des donnÃ©es brutes.
- **Cassandra** : Stockage des insights et des statistiques agrÃ©gÃ©es.
- **Streamlit Dashboard** : Interface de visualisation des donnÃ©es.

## ğŸ“ˆ Monitoring & RÃ©silience

- **Surveillance de Kafka** : VÃ©rification de la publication et consommation des messages.
- **Interface PySpark Streaming** : Suivi des tÃ¢ches Spark et gestion des erreurs.
- **MÃ©canisme de Reprise** : Gestion des pannes rÃ©seau et erreurs API.

## ğŸ“¬ Contactez-moi

- **Auteur** : Ali Rahiqi 
- **Email** : [ali123rahiqi@gmail.com](mailto:ali123rahiqi@gmail.com)

